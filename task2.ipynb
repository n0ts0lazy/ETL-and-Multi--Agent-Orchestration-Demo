{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57bbfe34",
   "metadata": {},
   "source": [
    "GPU scheduling and multi-agent orchestration concepts without actual GPUs by using simulation + CPU-based execution +   logical resource modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34fdba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Do not run this cell unless running the code on a local notebook. This line is for auto-reloading the code in the notebook whenever you make changes to the code files. \n",
    "# If you're running this code in a Jupyter notebook/ Google Colab, you can uncomment the line to enable auto-reloading of your code files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4b858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install  torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# Use it only if you are sure GPU is available and compatible with CUDA 12.1+. Otherwise, you can install the CPU version or the appropriate CUDA version for your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cecd83f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Environment configuration\n",
    "# -----------------------------\n",
    "gpu_available = True   # <-- change to True to enable GPU scheduling\n",
    "\n",
    "# What to do with GPU tasks when gpu_available=False:\n",
    "DOWNGRADE_GPU_TASKS_TO_CPU = True   # True = run them on CPU, False = reject them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8ec4f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CPU Info ===\n",
      "os: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39\n",
      "python: 3.12.3\n",
      "python_executable: /mnt/d/SRM_PG/workshop_info/env/bin/python\n",
      "cpu_logical_cores: 24\n",
      "\n",
      "=== GPU Info (driver) ===\n",
      "gpu_count: 1\n",
      "gpu_names: ['NVIDIA GeForce RTX 4070 Ti SUPER']\n",
      "\n",
      "=== Torch / CUDA Info ===\n",
      "torch_installed: True\n",
      "torch_version: 2.5.1+cu121\n",
      "torch_cuda_version: 12.1\n",
      "cuda_available: True\n",
      "cuda_device_count: 1\n",
      "device_names: ['NVIDIA GeForce RTX 4070 Ti SUPER']\n",
      "\n",
      "=== Mode ===\n",
      "gpu_available=True -> selected runtime device: cuda\n",
      "Reason: GPU detected and Torch reports CUDA is available.\n"
     ]
    }
   ],
   "source": [
    "import os, platform, sys, shutil, subprocess\n",
    "\n",
    "def get_cpu_info():\n",
    "    return {\n",
    "        \"os\": platform.platform(),\n",
    "        \"python\": platform.python_version(),\n",
    "        \"python_executable\": sys.executable,\n",
    "        \"cpu_logical_cores\": os.cpu_count() or 1,\n",
    "    }\n",
    "\n",
    "def get_gpu_info_nvidia():\n",
    "    if shutil.which(\"nvidia-smi\") is None:\n",
    "        return {\"gpu_count\": 0, \"gpu_names\": [], \"note\": \"nvidia-smi not found\"}\n",
    "\n",
    "    out = subprocess.check_output([\"nvidia-smi\", \"-L\"], text=True, stderr=subprocess.STDOUT)\n",
    "    gpu_lines = [ln.strip() for ln in out.splitlines() if ln.strip().startswith(\"GPU\")]\n",
    "    names = []\n",
    "    for ln in gpu_lines:\n",
    "        try:\n",
    "            names.append(ln.split(\":\", 1)[1].split(\"(UUID\", 1)[0].strip())\n",
    "        except Exception:\n",
    "            names.append(ln)\n",
    "    return {\"gpu_count\": len(gpu_lines), \"gpu_names\": names}\n",
    "\n",
    "def get_torch_gpu_info():\n",
    "    try:\n",
    "        import torch  # type: ignore\n",
    "\n",
    "        cuda_ok = torch.cuda.is_available()\n",
    "        info = {\n",
    "            \"torch_installed\": True,\n",
    "            \"torch_version\": torch.__version__,\n",
    "            \"torch_cuda_version\": getattr(torch.version, \"cuda\", None),\n",
    "            \"cuda_available\": cuda_ok,\n",
    "            \"cuda_device_count\": torch.cuda.device_count() if cuda_ok else 0,\n",
    "            \"device_names\": [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())] if cuda_ok else [],\n",
    "        }\n",
    "\n",
    "        # Extra: if CUDA isn't available, sometimes this helps diagnose in WSL\n",
    "        if not cuda_ok:\n",
    "            # Don't force GPU init; just provide env hints\n",
    "            info[\"env_CUDA_VISIBLE_DEVICES\"] = os.getenv(\"CUDA_VISIBLE_DEVICES\")\n",
    "            info[\"env_LD_LIBRARY_PATH\"] = os.getenv(\"LD_LIBRARY_PATH\")\n",
    "\n",
    "        return info\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"torch_installed\": False,\n",
    "            \"python_executable\": sys.executable,\n",
    "            \"error_type\": type(e).__name__,\n",
    "            \"error_message\": str(e),\n",
    "        }\n",
    "\n",
    "# ---- Run ----\n",
    "print(\"=== CPU Info ===\")\n",
    "cpu_info = get_cpu_info()\n",
    "for k, v in cpu_info.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "RUNTIME_DEVICE = \"cpu\"\n",
    "\n",
    "if not gpu_available:\n",
    "    print(\"\\n=== Mode ===\")\n",
    "    print(\"gpu_available=False -> forcing CPU only\")\n",
    "else:\n",
    "    print(\"\\n=== GPU Info (driver) ===\")\n",
    "    try:\n",
    "        gpu_info = get_gpu_info_nvidia()\n",
    "    except Exception as e:\n",
    "        gpu_info = {\"gpu_count\": 0, \"gpu_names\": [], \"error\": f\"{type(e).__name__}: {e}\"}\n",
    "    for k, v in gpu_info.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "    print(\"\\n=== Torch / CUDA Info ===\")\n",
    "    torch_info = get_torch_gpu_info()\n",
    "    for k, v in torch_info.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "    # Decision with clear reasoning\n",
    "    if gpu_info.get(\"gpu_count\", 0) == 0:\n",
    "        reason = \"No GPU detected by driver (nvidia-smi).\"\n",
    "        RUNTIME_DEVICE = \"cpu\"\n",
    "    elif not torch_info.get(\"torch_installed\", False):\n",
    "        reason = \"Torch is not importable in this Python environment.\"\n",
    "        RUNTIME_DEVICE = \"cpu\"\n",
    "    elif not torch_info.get(\"cuda_available\", False):\n",
    "        reason = \"Torch installed, but torch.cuda.is_available() is False (CUDA not usable).\"\n",
    "        RUNTIME_DEVICE = \"cpu\"\n",
    "    else:\n",
    "        reason = \"GPU detected and Torch reports CUDA is available.\"\n",
    "        RUNTIME_DEVICE = \"cuda\"\n",
    "\n",
    "    print(\"\\n=== Mode ===\")\n",
    "    print(f\"gpu_available=True -> selected runtime device: {RUNTIME_DEVICE}\")\n",
    "    print(\"Reason:\", reason)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c4bb842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "954df09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Hardware detection (only when gpu_available=True)\n",
    "# -----------------------------\n",
    "def detect_gpus_nvidia_smi() -> int:\n",
    "    if shutil.which(\"nvidia-smi\") is None:\n",
    "        return 0\n",
    "    try:\n",
    "        out = subprocess.check_output([\"nvidia-smi\", \"-L\"], stderr=subprocess.STDOUT, text=True)\n",
    "        lines = [ln.strip() for ln in out.splitlines() if ln.strip().startswith(\"GPU\")]\n",
    "        return len(lines)\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def torch_info_best_effort() -> dict:\n",
    "    try:\n",
    "        import torch  # type: ignore\n",
    "        return {\n",
    "            \"torch_installed\": True,\n",
    "            \"torch_version\": torch.__version__,\n",
    "            \"torch_cuda_version\": getattr(torch.version, \"cuda\", None),\n",
    "            \"cuda_available\": torch.cuda.is_available(),\n",
    "            \"cuda_device_count\": torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"torch_installed\": False, \"error\": f\"{type(e).__name__}: {e}\"}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c172cdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Logical resource model\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class Cluster:\n",
    "    cpu_cores: int\n",
    "    gpus: int\n",
    "    cpu_sem: asyncio.Semaphore = field(init=False)\n",
    "    gpu_sem: asyncio.Semaphore = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.cpu_sem = asyncio.Semaphore(self.cpu_cores)\n",
    "        self.gpu_sem = asyncio.Semaphore(self.gpus)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Task:\n",
    "    task_id: str\n",
    "    agent: str\n",
    "    duration_s: float\n",
    "    cpu_cores: int = 1\n",
    "    gpus: int = 0\n",
    "    kind: str = \"generic\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "762ecd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "def ts(t0: float) -> str:\n",
    "    return f\"{time.time() - t0:6.2f}s\"\n",
    "\n",
    "async def acquire_n(sem: asyncio.Semaphore, n: int):\n",
    "    for _ in range(n):\n",
    "        await sem.acquire()\n",
    "\n",
    "def release_n(sem: asyncio.Semaphore, n: int):\n",
    "    for _ in range(n):\n",
    "        sem.release()\n",
    "\n",
    "async def run_task(task: Task, cluster: Cluster, t0: float):\n",
    "    await acquire_n(cluster.cpu_sem, task.cpu_cores)\n",
    "    if task.gpus > 0:\n",
    "        await acquire_n(cluster.gpu_sem, task.gpus)\n",
    "\n",
    "    place = \"GPU\" if task.gpus > 0 else \"CPU\"\n",
    "    try:\n",
    "        print(f\"[{ts(t0)}] START {task.task_id:10} agent={task.agent:6} kind={task.kind:6} \"\n",
    "              f\"on={place} req(cpu={task.cpu_cores}, gpu={task.gpus})\")\n",
    "        await asyncio.sleep(task.duration_s)\n",
    "        print(f\"[{ts(t0)}] DONE  {task.task_id:10} agent={task.agent:6} on={place}\")\n",
    "    finally:\n",
    "        if task.gpus > 0:\n",
    "            release_n(cluster.gpu_sem, task.gpus)\n",
    "        release_n(cluster.cpu_sem, task.cpu_cores)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fea7b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Orchestrator (multi-agent)\n",
    "# -----------------------------\n",
    "class Orchestrator:\n",
    "    def __init__(self, cluster: Cluster):\n",
    "        self.cluster = cluster\n",
    "        self.queues: Dict[str, asyncio.Queue[Task]] = {}\n",
    "        self.agent_names: List[str] = []\n",
    "        self.running: List[asyncio.Task] = []\n",
    "\n",
    "    def register_agent(self, name: str):\n",
    "        if name not in self.queues:\n",
    "            self.queues[name] = asyncio.Queue()\n",
    "            self.agent_names.append(name)\n",
    "\n",
    "    async def submit(self, task: Task):\n",
    "        await self.queues[task.agent].put(task)\n",
    "\n",
    "    def _cleanup_done(self):\n",
    "        self.running = [t for t in self.running if not t.done()]\n",
    "\n",
    "    async def scheduler_loop(self, t0: float):\n",
    "        rr = 0\n",
    "        while True:\n",
    "            self._cleanup_done()\n",
    "\n",
    "            # stop when everything finished\n",
    "            if all(q.empty() for q in self.queues.values()) and len(self.running) == 0:\n",
    "                return\n",
    "\n",
    "            # round-robin agent pick\n",
    "            picked: Optional[str] = None\n",
    "            for _ in range(len(self.agent_names)):\n",
    "                name = self.agent_names[rr % len(self.agent_names)]\n",
    "                rr += 1\n",
    "                if not self.queues[name].empty():\n",
    "                    picked = name\n",
    "                    break\n",
    "\n",
    "            if picked is None:\n",
    "                await asyncio.sleep(0.02)\n",
    "                continue\n",
    "\n",
    "            task = await self.queues[picked].get()\n",
    "\n",
    "            # Admission control: reject impossible requests\n",
    "            if task.cpu_cores > self.cluster.cpu_cores:\n",
    "                print(f\"[{ts(t0)}] REJECT {task.task_id:10} cpu req too large \"\n",
    "                      f\"(cpu={task.cpu_cores}/{self.cluster.cpu_cores})\")\n",
    "                continue\n",
    "            if task.gpus > self.cluster.gpus:\n",
    "                print(f\"[{ts(t0)}] REJECT {task.task_id:10} gpu req too large \"\n",
    "                      f\"(gpu={task.gpus}/{self.cluster.gpus})\")\n",
    "                continue\n",
    "\n",
    "            self.running.append(asyncio.create_task(run_task(task, self.cluster, t0)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88ee37e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Logical Cluster Resources ===\n",
      "CPU cores (logical): 24\n",
      "gpu_available flag: True\n",
      "GPUs detected: 1 | GPUs used for scheduling: 1\n",
      "Torch info: {'torch_installed': True, 'torch_version': '2.5.1+cu121', 'torch_cuda_version': '12.1', 'cuda_available': True, 'cuda_device_count': 1}\n",
      "\n",
      "[  0.04s] SUBMIT A_etl_1    agent=agentA wants(cpu=2, gpu=0)\n",
      "[  0.04s] SUBMIT B_etl_1    agent=agentB wants(cpu=3, gpu=0)\n",
      "[  0.04s] SUBMIT C_cpu_1    agent=agentC wants(cpu=12, gpu=0)\n",
      "[  0.04s] START A_etl_1    agent=agentA kind=etl    on=CPU req(cpu=2, gpu=0)\n",
      "[  0.04s] START B_etl_1    agent=agentB kind=etl    on=CPU req(cpu=3, gpu=0)\n",
      "[  0.04s] START C_cpu_1    agent=agentC kind=cpujob on=CPU req(cpu=12, gpu=0)\n",
      "[  0.07s] SUBMIT A_trn_1    agent=agentA wants(cpu=2, gpu=1)\n",
      "[  0.07s] SUBMIT B_trn_1    agent=agentB wants(cpu=1, gpu=1)\n",
      "[  0.07s] SUBMIT C_inf_1    agent=agentC wants(cpu=1, gpu=1)\n",
      "[  0.08s] START A_trn_1    agent=agentA kind=train  on=GPU req(cpu=2, gpu=1)\n",
      "[  0.11s] SUBMIT A_inf_1    agent=agentA wants(cpu=1, gpu=1)\n",
      "[  1.04s] DONE  A_etl_1    agent=agentA on=CPU\n",
      "[  1.34s] DONE  B_etl_1    agent=agentB on=CPU\n",
      "[  1.65s] DONE  C_cpu_1    agent=agentC on=CPU\n",
      "[  2.29s] DONE  A_trn_1    agent=agentA on=GPU\n",
      "[  2.29s] START B_trn_1    agent=agentB kind=train  on=GPU req(cpu=1, gpu=1)\n",
      "[  4.29s] DONE  B_trn_1    agent=agentB on=GPU\n",
      "[  4.29s] START C_inf_1    agent=agentC kind=infer  on=GPU req(cpu=1, gpu=1)\n",
      "[  5.19s] DONE  C_inf_1    agent=agentC on=GPU\n",
      "[  5.19s] START A_inf_1    agent=agentA kind=infer  on=GPU req(cpu=1, gpu=1)\n",
      "[  6.29s] DONE  A_inf_1    agent=agentA on=GPU\n",
      "\n",
      "[  6.31s] ALL DONE\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Demo workload\n",
    "# -----------------------------\n",
    "async def agent_submitter(orch: Orchestrator, t0: float, tasks: List[Task]):\n",
    "    for task in tasks:\n",
    "        await orch.submit(task)\n",
    "        print(f\"[{ts(t0)}] SUBMIT {task.task_id:10} agent={task.agent:6} wants(cpu={task.cpu_cores}, gpu={task.gpus})\")\n",
    "        await asyncio.sleep(0.03)\n",
    "\n",
    "\n",
    "async def main():\n",
    "    t0 = time.time()\n",
    "\n",
    "    cpu_cores = os.cpu_count() or 1\n",
    "\n",
    "    # Respect the flag:\n",
    "    if not gpu_available:\n",
    "        real_gpus = 0\n",
    "        gpus_used = 0\n",
    "        torch_info = None\n",
    "    else:\n",
    "        real_gpus = detect_gpus_nvidia_smi()\n",
    "        gpus_used = real_gpus  # no simulation here; use what you actually have\n",
    "        torch_info = torch_info_best_effort()\n",
    "\n",
    "    print(\"=== Logical Cluster Resources ===\")\n",
    "    print(\"CPU cores (logical):\", cpu_cores)\n",
    "    print(\"gpu_available flag:\", gpu_available)\n",
    "    print(\"GPUs detected:\", real_gpus, \"| GPUs used for scheduling:\", gpus_used)\n",
    "    if torch_info is not None:\n",
    "        print(\"Torch info:\", torch_info)\n",
    "    print()\n",
    "\n",
    "    cluster = Cluster(cpu_cores=cpu_cores, gpus=gpus_used)\n",
    "    orch = Orchestrator(cluster)\n",
    "    for a in [\"agentA\", \"agentB\", \"agentC\"]:\n",
    "        orch.register_agent(a)\n",
    "\n",
    "    # Build tasks (GPU tasks will be downgraded/rejected when gpu_available=False)\n",
    "    def maybe_gpu(n=1):\n",
    "        if gpu_available:\n",
    "            return n\n",
    "        return 0 if DOWNGRADE_GPU_TASKS_TO_CPU else n  # if not downgrading, keep request and get rejected\n",
    "\n",
    "    tasks_A = [\n",
    "        Task(\"A_etl_1\", \"agentA\", duration_s=1.0, cpu_cores=2, gpus=0, kind=\"etl\"),\n",
    "        Task(\"A_trn_1\", \"agentA\", duration_s=2.2, cpu_cores=2, gpus=maybe_gpu(1), kind=\"train\"),\n",
    "        Task(\"A_inf_1\", \"agentA\", duration_s=1.1, cpu_cores=1, gpus=maybe_gpu(1), kind=\"infer\"),\n",
    "    ]\n",
    "    tasks_B = [\n",
    "        Task(\"B_etl_1\", \"agentB\", duration_s=1.3, cpu_cores=3, gpus=0, kind=\"etl\"),\n",
    "        Task(\"B_trn_1\", \"agentB\", duration_s=2.0, cpu_cores=1, gpus=maybe_gpu(1), kind=\"train\"),\n",
    "    ]\n",
    "    tasks_C = [\n",
    "        Task(\"C_cpu_1\", \"agentC\", duration_s=1.6, cpu_cores=cpu_cores // 2 or 1, gpus=0, kind=\"cpujob\"),\n",
    "        Task(\"C_inf_1\", \"agentC\", duration_s=0.9, cpu_cores=1, gpus=maybe_gpu(1), kind=\"infer\"),\n",
    "    ]\n",
    "\n",
    "    # If you chose \"reject\", print a one-liner so itâ€™s obvious\n",
    "    if (not gpu_available) and (not DOWNGRADE_GPU_TASKS_TO_CPU):\n",
    "        print(\"NOTE: gpu_available=False and DOWNGRADE_GPU_TASKS_TO_CPU=False -> GPU tasks will be rejected.\\n\")\n",
    "\n",
    "    await asyncio.gather(\n",
    "        agent_submitter(orch, t0, tasks_A),\n",
    "        agent_submitter(orch, t0, tasks_B),\n",
    "        agent_submitter(orch, t0, tasks_C),\n",
    "        orch.scheduler_loop(t0),\n",
    "    )\n",
    "\n",
    "    print(f\"\\n[{ts(t0)}] ALL DONE\")\n",
    "\n",
    "\n",
    "# ---- Run in notebook vs script ----\n",
    "await main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
