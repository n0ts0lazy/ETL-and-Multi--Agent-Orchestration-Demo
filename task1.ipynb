{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aede2d20",
   "metadata": {},
   "source": [
    "Secure ETL & Data Engineering to Build a controlled data ingestion and preprocessing pipeline.  \n",
    "  \n",
    "Data-Simulated datasets  \n",
    "sensor_logs.csv  \n",
    "mission_profile.json  \n",
    "maintenance_records.csv  \n",
    "  \n",
    "✔ Ingest multi-source data  \n",
    "✔ Merge using Equipment ID & Timestamp  \n",
    "✔ Handle missing sensor data  \n",
    "Feature engineering:  \n",
    "rolling averages  \n",
    "risk indicators  \n",
    "categorical encoding  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99456dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Do not run this cell unless running the code on a local notebook. This line is for auto-reloading the code in the notebook whenever you make changes to the code files. \n",
    "# If you're running this code in a Jupyter notebook/ Google Colab, you can uncomment the line to enable auto-reloading of your code files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e53a8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas numpy scikit-learn matplotlib sqlite3 #ipykernel # Uncomment this line if running in a Jupyter notebook locally\n",
    "# This line ensures that the necessary libraries are installed. If you're running this code in a Jupyter notebook/ Google Colab, you can uncomment the line to install the libraries directly from the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96c7843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c99873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 1) Load data\n",
    "# -----------------------\n",
    "sensor = pd.read_csv(\"./dataset/sensor_logs.csv\")\n",
    "sensor[\"timestamp\"] = pd.to_datetime(sensor[\"timestamp\"], errors=\"coerce\")\n",
    "sensor = sensor.dropna(subset=[\"timestamp\"]).sort_values([\"asset_id\", \"timestamp\"])\n",
    "\n",
    "mission = json.load(open(\"./dataset/mission_profile.json\", \"r\", encoding=\"utf-8\"))\n",
    "mission = pd.json_normalize(mission if isinstance(mission, list) else [mission])\n",
    "\n",
    "maint = pd.read_csv(\"./dataset/maintenance_records.csv\")\n",
    "maint[\"event_time\"] = pd.to_datetime(maint[\"event_time\"], errors=\"coerce\")\n",
    "maint = maint.dropna(subset=[\"event_time\"]).sort_values([\"asset_id\", \"event_time\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df68d30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 2) Feature engineering\n",
    "#    a) rolling averages (per asset)\n",
    "# -----------------------\n",
    "# Pick numeric sensor columns automatically (excluding id + time)\n",
    "num_sensor_cols = [\n",
    "    c for c in sensor.columns\n",
    "    if c not in [\"asset_id\", \"timestamp\"] and pd.api.types.is_numeric_dtype(sensor[c])\n",
    "]\n",
    "\n",
    "# Rolling mean over last 5 rows (simple + minimal)\n",
    "for c in num_sensor_cols:\n",
    "    sensor[f\"{c}_rollmean_5\"] = (\n",
    "        sensor.groupby(\"asset_id\")[c]\n",
    "        .rolling(window=5, min_periods=1)\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db2ee4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 2b) risk indicators\n",
    "# -----------------------\n",
    "# Simple thresholds (change to match your columns)\n",
    "THRESHOLDS = {\"temperature\": 85, \"vibration\": 4.5}\n",
    "\n",
    "risk_flag_cols = []\n",
    "for col, thr in THRESHOLDS.items():\n",
    "    if col in sensor.columns:\n",
    "        flag = f\"risk_{col}_high\"\n",
    "        sensor[flag] = (sensor[col] > thr).astype(int)\n",
    "        risk_flag_cols.append(flag)\n",
    "\n",
    "sensor[\"risk_score\"] = sensor[risk_flag_cols].sum(axis=1) if risk_flag_cols else 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fc5015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 3) Join datasets\n",
    "#    mission: plain left join\n",
    "#    maintenance: \"last maintenance event before this sensor timestamp\"\n",
    "# -----------------------\n",
    "df = sensor.merge(mission, on=\"asset_id\", how=\"left\")\n",
    "\n",
    "# merge_asof needs sorted frames\n",
    "df = df.sort_values([\"asset_id\", \"timestamp\"])\n",
    "maint = maint.sort_values([\"asset_id\", \"event_time\"])\n",
    "\n",
    "# asof join per asset (minimal & correct)\n",
    "out = []\n",
    "for asset_id, g in df.groupby(\"asset_id\", sort=False):\n",
    "    m = maint[maint[\"asset_id\"] == asset_id]\n",
    "    if m.empty:\n",
    "        out.append(g)\n",
    "        continue\n",
    "    out.append(\n",
    "        pd.merge_asof(\n",
    "            g.sort_values(\"timestamp\"),\n",
    "            m.sort_values(\"event_time\"),\n",
    "            left_on=\"timestamp\",\n",
    "            right_on=\"event_time\",\n",
    "            direction=\"backward\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "df = pd.concat(out, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a534d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 4) Categorical encoding (one-hot)\n",
    "# -----------------------\n",
    "cat_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n",
    "num_cols = [c for c in df.columns if c not in cat_cols]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols),\n",
    "        (\"num\", \"passthrough\", num_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "pipe = Pipeline([(\"preprocess\", preprocess)])\n",
    "\n",
    "X = pipe.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a490e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: feature names -> back to DataFrame\n",
    "feature_names = pipe.named_steps[\"preprocess\"].get_feature_names_out()\n",
    "X_df = pd.DataFrame(X, columns=feature_names)\n",
    "\n",
    "print(\"Final joined df shape:\", df.shape)\n",
    "print(\"Encoded matrix shape:\", X_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60e0acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pick one asset to inspect\n",
    "asset = df[\"asset_id\"].iloc[0]\n",
    "d = df[df[\"asset_id\"] == asset].sort_values(\"timestamp\")\n",
    "\n",
    "# 1) Missing values (top 15)\n",
    "na = df.isna().sum().sort_values(ascending=False).head(15)\n",
    "plt.figure()\n",
    "na.plot(kind=\"bar\")\n",
    "plt.title(\"Top missing-value columns\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63abbf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Raw vs rolling (example columns if present)\n",
    "for col in [\"temperature\", \"vibration\"]:\n",
    "    roll = f\"{col}_rollmean_5\"\n",
    "    if col in d.columns and roll in d.columns:\n",
    "        plt.figure()\n",
    "        plt.plot(d[\"timestamp\"], d[col], label=col)\n",
    "        plt.plot(d[\"timestamp\"], d[roll], label=roll)\n",
    "        plt.title(f\"{asset}: {col} vs rolling mean\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11de08a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Risk score distribution\n",
    "if \"risk_score\" in df.columns:\n",
    "    plt.figure()\n",
    "    plt.hist(df[\"risk_score\"].dropna(), bins=20)\n",
    "    plt.title(\"Risk score distribution\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddf38e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Categorical cardinality (top 10 per cat column)\n",
    "cat_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n",
    "for c in cat_cols[:5]:  # limit to first 5 to keep it quick\n",
    "    plt.figure()\n",
    "    df[c].value_counts(dropna=False).head(10).plot(kind=\"bar\")\n",
    "    plt.title(f\"Top categories: {c}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e107c173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to SQLite\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "DB_PATH = \"etl_pipeline.db\" # Database Object\n",
    "\n",
    "with sqlite3.connect(DB_PATH) as conn:  # Keeps the whole process in a single connection context to prevent manual exit at end\n",
    "    # Save joined + engineered data\n",
    "    df.to_sql(\"etl_features_raw\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "    # Save encoded feature matrix\n",
    "    X_df.to_sql(\"etl_features_encoded\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "    # (Optional) create indexes for faster lookup\n",
    "    conn.execute(\"CREATE INDEX IF NOT EXISTS idx_raw_asset_time ON etl_features_raw(asset_id, timestamp)\")\n",
    "    conn.execute(\"CREATE INDEX IF NOT EXISTS idx_raw_asset ON etl_features_raw(asset_id)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ace158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify by reading back\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "with sqlite3.connect(\"etl_pipeline.db\") as conn:\n",
    "    df2 = pd.read_sql(\"SELECT * FROM etl_features_raw LIMIT 5\", conn)\n",
    "    X2  = pd.read_sql(\"SELECT * FROM etl_features_encoded LIMIT 5\", conn)\n",
    "\n",
    "print(df2.head())\n",
    "print(X2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32e14ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save to CSV (optional)\n",
    "df.to_csv(\"final_joined.csv\", index=False)\n",
    "X_df.to_csv(\"final_features_encoded.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
